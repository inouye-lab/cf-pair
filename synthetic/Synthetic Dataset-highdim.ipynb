{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4e7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8dba048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000, 20]) torch.Size([3000, 80]) torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "D = 100\n",
    "batch_size = 128\n",
    "ratio = 0.2\n",
    "sigma_inv = 1\n",
    "sigma_spu = 0.1\n",
    "sigma_factor = [2, 20]\n",
    "\n",
    "\n",
    "\n",
    "Z_1 = torch.normal(0, 1, (N, int(D*ratio))).repeat(len(sigma_factor)+1,1)\n",
    "theta_1 = torch.normal(0, sigma_inv, (int(D*ratio), 1))\n",
    "Y = ((Z_1 @ theta_1).squeeze() > 0).to(torch.float32) # Y is a binary vector with 0 and 1\n",
    "Z_2 = [torch.normal(1/(D*(1-ratio))*(2*Y[:N]-1).unsqueeze(1).repeat(1,int(D*(1-ratio))), sigma_spu).to(torch.float32)]\n",
    "for d, factor in enumerate(sigma_factor):\n",
    "    Z_2.append((Z_2[0]-1/(D*(1-ratio))*(2*Y[:N]-1).unsqueeze(1))*factor+2/D*(2*Y[:N]-1).unsqueeze(1))\n",
    "Z_2 = torch.cat(Z_2)\n",
    "print(Z_1.shape, Z_2.shape, Y.shape)\n",
    "Z = torch.cat((Z_1,Z_2),1)\n",
    "while True:\n",
    "    matrix = torch.rand(D, D)  # Create a DxD matrix with random values between 0 and 1\n",
    "    if torch.linalg.matrix_rank(matrix) == D:\n",
    "        Q, R = torch.linalg.qr(matrix)\n",
    "        break\n",
    "X = Z @ Q\n",
    "\n",
    "X_train, X_test = X[:N*(len(sigma_factor))], X[N*(len(sigma_factor)):]\n",
    "Y_train, Y_test = Y[:N*(len(sigma_factor))], Y[N*(len(sigma_factor)):]\n",
    "Z_train, Z_test = Z[:N*(len(sigma_factor))], Z[N*(len(sigma_factor)):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... (Your existing code for generating X, Y, Z) ...\n",
    "\n",
    "\n",
    "\n",
    "class PairedDomainDataset(Dataset):\n",
    "    def __init__(self, X, Y, Z, domain_labels):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.domain_labels = domain_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) // 2  # Half the size since we're pairing\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a pair of indices from the same domain\n",
    "        idx1 = idx\n",
    "        idx2 = idx + len(self.X) // 2\n",
    "        return self.X[idx1], self.Y[idx1], self.X[idx2], self.Y[idx2]\n",
    "\n",
    "# Generate domain labels (assuming the first half is one domain, the second half is the other)\n",
    "domain_labels = np.concatenate([np.zeros(N * len(sigma_factor)), np.ones(N)])\n",
    "\n",
    "# Create the paired dataset\n",
    "paired_dataset = PairedDomainDataset(X_train, Y_train, Z_train, domain_labels)\n",
    "\n",
    "\n",
    "paired_loader = DataLoader(paired_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5a8cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 100]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f218b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.783"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(X_train, Y_train)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88ada8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.783"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(Z_train, Y_train)\n",
    "clf.score(Z_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4d95e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oracle\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(Z_train[:,:int(D*ratio)], Y_train)\n",
    "clf.score(Z_test[:,:int(D*ratio)], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcbd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, D_in):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear_1 = nn.Linear(D_in, D_in)\n",
    "        self.linear_2 = nn.Linear(D_in, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.featurizer(x))\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, x):\n",
    "        x = self.linear_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc51f0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4814999997615814 0.5049999952316284\n",
      "0.48100000619888306 0.5040000081062317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y, Y_train)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m y_test \u001b[38;5;241m=\u001b[39m model(X_test)\n",
      "File \u001b[0;32m/local/scratch/a/bai116/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/scratch/a/bai116/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for repeat in range(5):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    for n in range(5000): \n",
    "        y = model(Z_train)\n",
    "        loss = criterion(y, Y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "59dfa783",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[229], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m y_1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclassifier(z_1)\n\u001b[1;32m     13\u001b[0m y_2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclassifier(z_2)\n\u001b[0;32m---> 14\u001b[0m loss_1 \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(z_1.shape,z_2.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss_2 \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnorm(z_1\u001b[38;5;241m-\u001b[39mz_2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(z_1)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/local/scratch/a/bai116/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/local/scratch/a/bai116/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/scratch/a/bai116/miniconda3/envs/benchmark/lib/python3.8/site-packages/torch/nn/functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3096\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 2000\n",
    "for repeat in range(5):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    lmda = 1000\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        z_1 = model.featurizer(X_train[:N])\n",
    "        z_2 = model.featurizer(X_train[N:])\n",
    "        y_1 = model.classifier(z_1)\n",
    "        y_2 = model.classifier(z_2)\n",
    "        loss_1 = criterion(torch.cat((y_1,y_2)), Y_train)\n",
    "        # print(z_1.shape,z_2.shape)\n",
    "        loss_2 = (torch.norm(z_1-z_2, p=2) / len(z_1)) ** 2\n",
    "        # print(loss_1,loss_2)\n",
    "        loss = loss_1 + lmda * loss_2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc60fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9929999709129333 0.9900000095367432\n",
      "0.9909999966621399 0.9900000095367432\n",
      "0.9904999732971191 0.9919999837875366\n",
      "0.996999979019165 0.9959999918937683\n",
      "0.9919999837875366 0.9950000047683716\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "for repeat in range(5):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    lmda = 100\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X_1, Y_1, X_2, Y_2) in enumerate(paired_loader):\n",
    "\n",
    "            loss_1 = criterion(torch.cat((y_1,y_2)), torch.cat((Y_1,Y_2)))\n",
    "            # print(z_1.shape,z_2.shape)\n",
    "            loss_2 = (torch.norm(z_1-z_2, p=2) / len(z_1))\n",
    "            # print(loss_1,loss_2)\n",
    "            loss = loss_1 + lmda * loss_2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64ae8ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9789999723434448 0.8700000047683716\n",
      "0.9764999747276306 0.8690000176429749\n",
      "0.9764999747276306 0.8809999823570251\n",
      "0.9674999713897705 0.8489999771118164\n",
      "0.9695000052452087 0.8519999980926514\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "shot = 60\n",
    "for repeat in range(5):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    lmda = 100\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X, Y) in enumerate(loader):\n",
    "            X= torch.cat((X_train[0:shot], X, X_train[N:N+shot]))\n",
    "            Y = torch.cat((Y_train[0:shot], Y, Y_train[N:N+shot]))\n",
    "            z = model.featurizer(X)\n",
    "            y = model.classifier(z)\n",
    "            loss_1 = criterion(y, Y)\n",
    "            loss_2 = (torch.norm(z[:shot]-z[-shot:], p=2) / len(z_1))\n",
    "            loss = loss_1 + lmda * loss_2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
