{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4e7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dba048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 80]) torch.Size([300, 20]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "D = 100\n",
    "batch_size = 128\n",
    "ratio = 0.8 # ratio of the number of parent features \n",
    "sigma_inv = 1\n",
    "sigma_spu = 0.1\n",
    "sigma_factor = [2, 100]\n",
    "\n",
    "\n",
    "Z_1 = torch.normal(0, 1, (N, int(round(D*ratio)))).repeat(len(sigma_factor)+1,1)\n",
    "theta_1 = torch.normal(0, sigma_inv, (int(round(D*ratio)), 1))\n",
    "Y = ((Z_1 @ theta_1).squeeze() > 0).to(torch.float32) # Y is a binary vector with 0 and 1\n",
    "Z_2 = [torch.normal(1/(D*(1-ratio))*(2*Y[:N]-1).unsqueeze(1).repeat(1,int(round(D*(1-ratio)))), sigma_spu).to(torch.float32)]\n",
    "# Z_2 = [torch.normal((2*Y[:N]-1).unsqueeze(1).repeat(1,int(round(D*(1-ratio)))), sigma_spu).to(torch.float32)]\n",
    "for d, factor in enumerate(sigma_factor):\n",
    "    Z_2.append((Z_2[0]-1/(D*(1-ratio))*(2*Y[:N]-1).unsqueeze(1))*factor+2/D*(2*Y[:N]-1).unsqueeze(1))\n",
    "Z_2 = torch.cat(Z_2)\n",
    "print(Z_1.shape, Z_2.shape, Y.shape)\n",
    "Z = torch.cat((Z_1,Z_2),1)\n",
    "while True:\n",
    "    matrix = torch.randn(D, D)  # Create a DxD matrix with random values between 0 and 1\n",
    "    if torch.linalg.matrix_rank(matrix) == D:\n",
    "        Q, R = torch.linalg.qr(matrix)\n",
    "        break\n",
    "X = Z @ Q\n",
    "\n",
    "X_train, X_test = X[:N*(len(sigma_factor))], X[N*(len(sigma_factor)):]\n",
    "Y_train, Y_test = Y[:N*(len(sigma_factor))], Y[N*(len(sigma_factor)):]\n",
    "Z_train, Z_test = Z[:N*(len(sigma_factor))], Z[N*(len(sigma_factor)):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... (Your existing code for generating X, Y, Z) ...\n",
    "\n",
    "\n",
    "\n",
    "class PairedDomainDataset(Dataset):\n",
    "    def __init__(self, X, Y, Z, domain_labels):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.domain_labels = domain_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) // 2  # Half the size since we're pairing\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a pair of indices from the same domain\n",
    "        idx1 = idx\n",
    "        idx2 = idx + len(self.X) // 2\n",
    "        return self.X[idx1], self.Y[idx1], self.X[idx2], self.Y[idx2]\n",
    "\n",
    "# Generate domain labels (assuming the first half is one domain, the second half is the other)\n",
    "domain_labels = np.concatenate([np.zeros(N * len(sigma_factor)), np.ones(N)])\n",
    "\n",
    "# Create the paired dataset\n",
    "paired_dataset = PairedDomainDataset(X_train, Y_train, Z_train, domain_labels)\n",
    "\n",
    "\n",
    "paired_loader = DataLoader(paired_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f218b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(X_train, Y_train)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ada8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(Z_train, Y_train)\n",
    "clf.score(Z_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d95e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oracle\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(Z_train[:,:int(round(D*ratio))], Y_train)\n",
    "clf.score(Z_test[:,:int(round(D*ratio))], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcbd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, D_in):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear_1 = nn.Linear(D_in, D_in)\n",
    "        self.linear_2 = nn.Linear(D_in, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.featurizer(x))\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, x):\n",
    "        x = self.linear_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc51f0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.7099999785423279\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.7099999785423279\n",
      "1.0 0.699999988079071\n",
      "1.0 0.6899999976158142\n",
      "1.0 0.699999988079071\n",
      "1.0 0.7799999713897705\n",
      "1.0 0.7099999785423279\n",
      "1.0 0.7300000190734863\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.6499999761581421\n",
      "1.0 0.7300000190734863\n",
      "1.0 0.699999988079071\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.6800000071525574\n",
      "1.0 0.7099999785423279\n",
      "1.0 0.7699999809265137\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.7599999904632568\n",
      "---\n",
      "0.718500018119812 0.030482955276966095\n"
     ]
    }
   ],
   "source": [
    "# ERM\n",
    "epoch = 100\n",
    "\n",
    "repeats = 20\n",
    "sum_in = []\n",
    "sum_out = []\n",
    "for repeat in range(repeats):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X, Y) in enumerate(loader):\n",
    "            y = model(X)\n",
    "            loss = criterion(y, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    sum_in.append(float(((y_train>0.5)==Y_train).sum()/len(y_train)))\n",
    "    sum_out.append(float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "\n",
    "print(\"---\")\n",
    "print(torch.tensor(sum_out).mean().item(), torch.tensor(sum_out).std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc60fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "---\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# CF Pair\n",
    "epoch = 100\n",
    "\n",
    "repeats = 20\n",
    "sum_in = 0\n",
    "sum_out = 0\n",
    "for repeat in range(repeats):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    lmda = 1000\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X_1, Y_1, X_2, Y_2) in enumerate(paired_loader):\n",
    "            z_1 = model.featurizer(X_1)\n",
    "            z_2 = model.featurizer(X_2)\n",
    "            y_1 = model.classifier(z_1)\n",
    "            y_2 = model.classifier(z_2)\n",
    "            loss_1 = criterion(torch.cat((y_1,y_2)), torch.cat((Y_1,Y_2)))\n",
    "            # print(z_1.shape,z_2.shape)\n",
    "            loss_2 = (torch.norm(z_1-z_2, p=2) / len(z_1)) ** 2\n",
    "            # print(loss_1,loss_2)\n",
    "            loss = loss_1 + lmda * loss_2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    sum_in +=  float(((y_train>0.5)==Y_train).sum()/len(y_train))\n",
    "    sum_out += float(((y_test>0.5)==Y_test).sum()/len(y_test))\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "    \n",
    "print(\"---\")\n",
    "print(sum_in/repeats, sum_out/repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ae8ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6225000023841858 0.02633288875222206\n",
      "0.6599999666213989 0.042052607983350754\n",
      "0.6584999561309814 0.036745939403772354\n",
      "0.6480000615119934 0.050533995032310486\n",
      "0.6754999756813049 0.03347818925976753\n",
      "0.687999963760376 0.05970100313425064\n",
      "0.718000054359436 0.048297323286533356\n",
      "0.724000096321106 0.03250911086797714\n",
      "0.7584999799728394 0.03313051909208298\n",
      "0.7605000734329224 0.040584541857242584\n",
      "0.7760000228881836 0.04827552288770676\n",
      "0.7854999899864197 0.05716780200600624\n",
      "0.7994999885559082 0.045707885175943375\n",
      "0.8255001306533813 0.07416021078824997\n",
      "0.8024999499320984 0.04972291737794876\n",
      "0.8639999628067017 0.0578928217291832\n",
      "0.8515000343322754 0.07569155842065811\n",
      "0.9100000262260437 0.06415442377328873\n",
      "0.9545000195503235 0.06361686438322067\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Few shot CF Pair\n",
    "epoch = 100\n",
    "\n",
    "repeats = 20\n",
    "\n",
    "\n",
    "shots = np.arange(1,100)\n",
    "for shot in shots:\n",
    "    sum_in = []\n",
    "    sum_out = []\n",
    "\n",
    "    for repeat in range(repeats):\n",
    "        model = LR(D)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        lmda = 2000\n",
    "        model.train()\n",
    "        for i in range(epoch):\n",
    "            for j, (X, Y) in enumerate(loader):\n",
    "                X= torch.cat((X_train[0:shot], X, X_train[N:N+shot]))\n",
    "                Y = torch.cat((Y_train[0:shot], Y, Y_train[N:N+shot]))\n",
    "                z = model.featurizer(X)\n",
    "                y = model.classifier(z)\n",
    "                loss_1 = criterion(y, Y)\n",
    "                loss_2 = (torch.norm(z[:shot]-z[-shot:], p=2) / shot)\n",
    "                loss = loss_1 + lmda * loss_2\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        model.eval()\n",
    "        y_test = model(X_test)\n",
    "        y_train = model(X_train)\n",
    "        sum_in.append(float(((y_train>0.5)==Y_train).sum()/len(y_train)))\n",
    "        sum_out.append(float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "        # print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "\n",
    "    print(torch.tensor(sum_out).mean().item(), torch.tensor(sum_out).std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRM\n",
    "\n",
    "repeats = 20\n",
    "sum_in = 0\n",
    "sum_out = 0\n",
    "\n",
    "\n",
    "import torch.autograd as autograd\n",
    "\n",
    "scale = torch.tensor(1.).requires_grad_()\n",
    "\n",
    "\n",
    "def irm_penalty(loss_0, loss_1):\n",
    "    grad_0 = autograd.grad(loss_0.mean(), [scale], create_graph=True)[0]\n",
    "    grad_1 = autograd.grad(loss_1.mean(), [scale], create_graph=True)[0]\n",
    "    result = torch.sum(grad_0 * grad_1)\n",
    "    del grad_0, grad_1\n",
    "    return result\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    penalty_weight = 1\n",
    "\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X_1, Y_1, X_2, Y_2) in enumerate(paired_loader):\n",
    "            z_1 = model.featurizer(X_1)\n",
    "            z_2 = model.featurizer(X_2)\n",
    "            y_1 = model.classifier(z_1)\n",
    "            y_2 = model.classifier(z_2)\n",
    "            loss_1 = criterion(y_1*scale, Y_1)\n",
    "            loss_2 = criterion(y_2*scale, Y_2)\n",
    "            loss_3 = irm_penalty(loss_1, loss_2)\n",
    "            # print(loss_1,loss_2)\n",
    "            loss = loss_1 + lmda * loss_2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    sum_in +=  float(((y_train>0.5)==Y_train).sum()/len(y_train))\n",
    "    sum_out += float(((y_test>0.5)==Y_test).sum()/len(y_test))\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "\n",
    "print(\"---\")\n",
    "print(sum_in/repeats, sum_out/repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afaae76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
